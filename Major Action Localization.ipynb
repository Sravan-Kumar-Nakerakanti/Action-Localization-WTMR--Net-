{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edd8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import time\n",
    "from IPython.display import display\n",
    "import PIL.Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "model = load_model('WTMRNet2.h5')\n",
    "\n",
    "categories = ['wave', 'walk', 'turn', 'throw', 'talk', 'stand', 'smile', 'situp', 'sit', 'shake_hands', 'run', 'push', 'punch', 'pour',\n",
    "              'pick', 'laugh', 'jump', 'hug', 'hit', 'handstand', 'fall_floor', 'eat', 'drink', 'dribble', 'climb_stairs', 'climb', 'clap',\n",
    "              'chew', 'catch', 'brush_hair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fcd118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_fullbody.xml')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "object_detector = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1).to(device)\n",
    "object_detector.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_resized = cv2.resize(frame_gray, (64, 64))\n",
    "    frame_normalized = frame_resized / 255.0\n",
    "    return frame_normalized.reshape(1, 64, 64, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_person(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    persons = haar_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    return persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc447bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(frame):\n",
    "    small_frame = cv2.resize(frame, (320, 240))  \n",
    "    image = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        predictions = object_detector(image_tensor)[0]\n",
    "    \n",
    "    detected_objects = []\n",
    "    for label, score, box in zip(predictions['labels'], predictions['scores'], predictions['boxes']):\n",
    "        if score > 0.5:  \n",
    "            label_name = COCO_LABELS.get(label.item(), f\"Unknown ({label.item()})\")  \n",
    "            detected_objects.append((label_name, box.cpu().numpy()))\n",
    "    \n",
    "    return detected_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a067aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_action_frames(video_path, start_time, end_time, fps):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_indices = [int(start_time * fps), int((start_time + end_time) / 2 * fps), int(end_time * fps)]\n",
    "    extracted_frames = []\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            extracted_frames.append(frame)\n",
    "    cap.release()\n",
    "    return extracted_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video1(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = 0\n",
    "    persons_info = {}\n",
    "    detected_objects = []\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        person_boxes = detect_person(frame)\n",
    "        \n",
    "        if frame_count % 10 == 0:  \n",
    "            detected_objects = detect_objects(frame)\n",
    "        \n",
    "        environment_context = \", \".join([obj[0] for obj in detected_objects]) if detected_objects else \"Unknown\"\n",
    "        \n",
    "        for idx, (x, y, w, h) in enumerate(person_boxes):\n",
    "            x1, y1, x2, y2 = x, y, x + w, y + h\n",
    "            cropped_person = frame[y1:y2, x1:x2]\n",
    "            input_frame = preprocess_frame(cropped_person)\n",
    "            predictions = model.predict(input_frame, verbose=0)\n",
    "            predicted_label = categories[np.argmax(predictions)]\n",
    "            confidence = np.max(predictions)\n",
    "            \n",
    "            if confidence > 0.2:\n",
    "                timestamp = frame_count / fps\n",
    "                \n",
    "                if idx not in persons_info:\n",
    "                    persons_info[idx] = {}\n",
    "                if predicted_label not in persons_info[idx]:\n",
    "                    persons_info[idx][predicted_label] = {'count': 0, 'timestamps': [], 'start_time': None, 'end_time': None, 'context': environment_context}\n",
    "                \n",
    "                persons_info[idx][predicted_label]['timestamps'].append(timestamp)\n",
    "                persons_info[idx][predicted_label]['count'] += 1\n",
    "                if persons_info[idx][predicted_label]['start_time'] is None:\n",
    "                    persons_info[idx][predicted_label]['start_time'] = timestamp\n",
    "                persons_info[idx][predicted_label]['end_time'] = timestamp\n",
    "        \n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    \n",
    "    total_persons = len(persons_info)\n",
    "    total_actions = sum(len(actions) for actions in persons_info.values())\n",
    "    total_duration = frame_count / fps\n",
    "    \n",
    "    if total_persons == 1:\n",
    "        print(f\"Total Actions Detected: {total_actions}\")\n",
    "        print(f\"Total Duration of video: {total_duration:.2f}s\")\n",
    "        \n",
    "        for person_id, actions in persons_info.items():\n",
    "            print(f\"\\nPerson {person_id + 1}:\")\n",
    "            summary = {}\n",
    "            \n",
    "            for action, details in actions.items():\n",
    "                start_time, end_time = details['start_time'], details['end_time']\n",
    "                duration = end_time - start_time\n",
    "                extracted_frames = extract_action_frames(video_path, start_time, end_time, fps)\n",
    "                \n",
    "                summary[action] = {\n",
    "                    'count': details['count'], 'start_time': start_time, 'end_time': end_time,\n",
    "                    'duration': duration, 'frames': extracted_frames, 'context': details['context']\n",
    "                }\n",
    "                \n",
    "                print(f\"- {action} → Timestamp: {start_time:.2f}s | Duration: {duration:.2f}s ({start_time:.2f}s - {end_time:.2f}s)\")\n",
    "                print(f\"Context: {details['context']}\")\n",
    "                for frame in extracted_frames:\n",
    "                    _, buffer = cv2.imencode('.jpg', frame)\n",
    "                    display(PIL.Image.open(io.BytesIO(buffer)))\n",
    "            \n",
    "            max_action = max(summary, key=lambda x: summary[x]['count'])\n",
    "            min_action = min(summary, key=lambda x: summary[x]['count'])\n",
    "            longest_action = max(summary, key=lambda x: summary[x]['duration'])\n",
    "            shortest_action = min(summary, key=lambda x: summary[x]['duration'])\n",
    "        \n",
    "            print(f\"Most Frequent Action: {max_action} ({summary[max_action]['count']} times)\")\n",
    "            print(f\"Least Frequent Action: {min_action} ({summary[min_action]['count']} times)\")\n",
    "            print(f\"Action with Longest Duration: {longest_action} ({summary[longest_action]['duration']:.2f}s)\")\n",
    "            print(f\"Action with Shortest Duration: {shortest_action} ({summary[shortest_action]['duration']:.2f}s)\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Total Persons Detected: {total_persons}\")\n",
    "        print(f\"Total Actions Detected: {total_actions}\")\n",
    "        print(f\"Total Duration of video: {total_duration:.2f}s\")\n",
    "        \n",
    "        for person_id, actions in persons_info.items():\n",
    "            print(f\"\\nPerson {person_id + 1}:\")\n",
    "            summary = {}\n",
    "            \n",
    "            for action, details in actions.items():\n",
    "                start_time, end_time = details['start_time'], details['end_time']\n",
    "                duration = end_time - start_time\n",
    "                extracted_frames = extract_action_frames(video_path, start_time, end_time, fps)\n",
    "                \n",
    "                summary[action] = {\n",
    "                    'count': details['count'], 'start_time': start_time, 'end_time': end_time,\n",
    "                    'duration': duration, 'frames': extracted_frames, 'context': details['context']\n",
    "                }\n",
    "                \n",
    "                print(f\"- {action} → Timestamp: {start_time:.2f}s | Duration: {duration:.2f}s ({start_time:.2f}s - {end_time:.2f}s)\")\n",
    "                print(f\"Context: {details['context']}\")\n",
    "                for frame in extracted_frames:\n",
    "                    _, buffer = cv2.imencode('.jpg', frame)\n",
    "                    display(PIL.Image.open(io.BytesIO(buffer)))\n",
    "            \n",
    "            max_action = max(summary, key=lambda x: summary[x]['count'])\n",
    "            min_action = min(summary, key=lambda x: summary[x]['count'])\n",
    "            longest_action = max(summary, key=lambda x: summary[x]['duration'])\n",
    "            shortest_action = min(summary, key=lambda x: summary[x]['duration'])\n",
    "        \n",
    "            print(f\"Most Frequent Action: {max_action} ({summary[max_action]['count']} times)\")\n",
    "            print(f\"Least Frequent Action: {min_action} ({summary[min_action]['count']} times)\")\n",
    "            print(f\"Action with Longest Duration: {longest_action} ({summary[longest_action]['duration']:.2f}s)\")\n",
    "            print(f\"Action with Shortest Duration: {shortest_action} ({summary[shortest_action]['duration']:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fadec",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video1('walking and sitting.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b3c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video1('standup.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767a1fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_video1('SQA1.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb244707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
